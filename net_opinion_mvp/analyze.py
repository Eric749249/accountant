# analyze.py â€” GPT APIã§ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ(-1/0/+1)ã¨ãƒˆãƒ”ãƒƒã‚¯ã‚’è¿”ã™å®Ÿè£…ï¼ˆèªå¥è¾æ›¸ã¯ä½¿ã‚ãªã„ï¼‰
import os, json, time, math, re
from typing import List, Dict
import pandas as pd

try:
    from openai import OpenAI
except Exception as e:
    raise RuntimeError("openai ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`pip install openai` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚") from e

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚PowerShellã§ `$env:OPENAI_API_KEY = \"...\"` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚³ã‚¹ãƒˆãƒ»é€Ÿåº¦ã®ãƒãƒ©ãƒ³ã‚¹ã§å°å‹ã‚’æ¨å¥¨ï¼‰
DEFAULT_MODEL = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")

TOPIC_LABELS = [
    "æ”¿ç­–", "äººæ ¼", "å¤–äº¤", "çµŒæ¸ˆ",
    "å›½ä¼šé‹å–¶", "å…šæ´¾æ”¯æŒ", "ãƒ¡ãƒ‡ã‚£ã‚¢", "å€«ç†",
    "ãã®ä»–",
]

SYSTEM_PROMPT = (
    "You are a precise political stance classifier for Japanese YouTube comments. "
    "Your task is to determine the user's stance toward Prime Minister Sanae Takaichi. "
    "For each input text, return a JSON object with key `results`, containing an array aligned to inputs. "
    "Each item MUST have: `sentiment` (integer -1/0/1) and `topic` "
    "(one of: æ”¿ç­–, äººæ ¼, å¤–äº¤, çµŒæ¸ˆ, å›½ä¼šé‹å–¶, å…šæ´¾æ”¯æŒ, ãƒ¡ãƒ‡ã‚£ã‚¢, å€«ç†, ãã®ä»–). "
    "Output strictly valid JSON only, with no explanations.\n\n"

    "=== ENTITY DEFINITION ===\n"
    "Target person = Prime Minister Sanae Takaichi. "
    "She may be referred to by various expressions such as ã€Œé¦–ç›¸ã€, ã€Œé«˜å¸‚é¦–ç›¸ã€, ã€Œé«˜å¸‚ã•ã‚“ã€, ã€Œæ—©è‹—ã€, ã€Œé«˜å¸‚æ°ã€, etc. "
    "All such variations, pronouns, or implied references refer to the same person.\n\n"

    "=== SENTIMENT DEFINITION ===\n"
    "`sentiment` represents the stance toward Prime Minister Takaichi:\n"
    "-1 â†’ clear opposition / disapproval / criticism / rejection of Takaichi.\n"
    " 0 â†’ neutral / unclear / unrelated / general political sarcasm not directed at her.\n"
    " 1 â†’ clear support / approval / praise / defense of Takaichi.\n\n"

    "When Takaichi is being criticized, questioned, overworked, or treated unfairly, "
    "any expression of sympathy, compassion, or defense toward her must be classified as 1 "
    "even if her name is not directly mentioned (e.g. ã€Œç·ç†ã€, ã€Œå½¼å¥³ã€, ã€Œã„ã˜ã‚ã€, ã€Œã‹ã‚ã„ãã†ã€, "
    "ã€Œä¼‘ã¾ã›ã¦ã€, ã€Œãƒ–ãƒ©ãƒƒã‚¯åŠ´åƒã€, ã€Œå€’ã‚Œãªã„ã§ã€). "
    "When other people or institutions (Diet members, opposition parties, media, questioners) "
    "are criticized while Takaichi is protected, classify it as 1.\n\n"

    "=== SENTIMENT DECISION RULES (apply in order) ===\n"
    "Purpose: Classify each comment as Support (1), Neutral (0), or Oppose (-1) toward Prime Minister Sanae Takaichi.\n\n"

    "--- 1. Clear Positive (1) ---\n"
    "1) Direct praise, support, sympathy, or protection toward Takaichi "
    "(e.g., ã€Œé«˜å¸‚ã•ã‚“é ‘å¼µã£ã¦ã€ã€Œç·ç†ã‚’å®ˆã‚Œã€ã€Œã‹ã‚ã„ãã†ã€ã€Œä¼‘ã¾ã›ã¦ã‚ã’ã¦ã€) â†’ 1.\n"
    "2) Comments expressing anger or frustration toward those who overwork, mistreat, attack, or mock Takaichi "
    "(e.g., é‡å…š, ãƒ¡ãƒ‡ã‚£ã‚¢, è³ªå•è€…) should be classified as 1, "
    "even when written with strong or aggressive tone or emojis such as ã€Œã‚¢ãƒ›ã€ã€Œãã ã‚‰ãªã„ã€ã€Œç„¡è¦–ã§ã„ã„ã€ã€ŒğŸ’¢ã€, "
    "as long as the anger is clearly NOT directed at Takaichi herself.\n"
    "3) Criticism of opposition parties or the media that clearly functions as a defense of Takaichi or the ruling party "
    "(e.g., ã€Œãã ã‚‰ãªã„è¿½åŠã«ä»˜ãåˆã†å¿…è¦ãªã„ã€ã€Œé‡å…šã®ã„ã˜ã‚è³ªå•ã²ã©ã„ã€) â†’ 1.\n"
    "4) Explicit statements that Takaichi should continue in office, become or remain prime minister, "
    "or is preferable to other candidates (e.g., ã€Œé«˜å¸‚ç¶šæŠ•ã§ã€ã€Œæ¬¡ã¯é«˜å¸‚ã§è¡Œãã¹ãã€) â†’ 1.\n\n"

    "--- 2. Clear Negative (-1) ---\n"
    "5) Only classify as -1 when there is direct criticism, mockery, or rejection aimed at Takaichi herself, "
    "using her name, title, or clear reference (e.g., ã€Œé«˜å¸‚ã¯ã„ã‚‰ãªã„ã€ã€Œé«˜å¸‚ã¯ç„¡ç†ã€ã€Œé«˜å¸‚ãŒæœ€æ‚ªã€ã€Œã“ã®ç·ç†ã¯çµ‚ã‚ã£ã¦ã‚‹ã€). â†’ -1.\n"
    "6) Strong criticism of the LDP or ruling bloc should be classified as -1 only when the criticism explicitly "
    "includes or targets Takaichi as part of the problem (e.g., ã€Œé«˜å¸‚ã‚‚å«ã‚ã¦è‡ªæ°‘ã¯å…¨éƒ¨ãƒ€ãƒ¡ã€). "
    "If Takaichi is not clearly included, do NOT assume -1.\n\n"

    "--- 3. Indirect or context-dependent cases ---\n"
    "7) Support for other LDP politicians or other prime minister candidates (e.g., Kishida, Ishiba, Motegi) "
    "without any clear negative or positive statement about Takaichi â†’ 0.\n"
    "8) Support for opposition parties or non-LDP politicians (e.g., ç«‹æ†², å…±ç”£, ç¶­æ–°) "
    "without any clear negative or positive statement about Takaichi â†’ 0.\n"
    "9) Criticism of opposition parties only (e.g., ã€Œç«‹æ†²ã¯ãã ã‚‰ãªã„è³ªå•ã°ã‹ã‚Šã€) with no clear mention of Takaichi "
    "should usually be 0, unless it clearly functions as a defense of her as in Rule 3.\n"
    "10) General frustration, satire, or complaints about politics or society as a whole, "
    "when it is unclear whether Takaichi is supported or opposed â†’ 0.\n"
    "11) When both positive and negative signals toward Takaichi appear but the overall stance is unclear or contradictory â†’ 0.\n\n"

    "--- 4. Tie-breaking policy ---\n"
    "12) If there is clear evidence for 1 and no explicit attack on Takaichi, choose 1.\n"
    "13) Only choose -1 when there is clear and direct negative language aimed at Takaichi herself. "
    "In all other ambiguous cases, choose 0.\n\n"

    "=== TOPIC DEFINITION ===\n"
    "Choose the single most relevant category for the main discussion in the text.\n"
    "Prefer a specific category over 'ãã®ä»–' whenever applicable.\n"
    "- æ”¿ç­–: å€‹åˆ¥æ”¿ç­–ã‚„åˆ¶åº¦ã€æ³•æ”¹æ­£ã€è¡Œæ”¿ã®æ‰“ã¡æ‰‹ã®æ˜¯é\n"
    "- äººæ ¼: äººæŸ„ãƒ»å§¿å‹¢ãƒ»è¨€å‹•ãƒ»ãƒãƒŠãƒ¼ãƒ»ä½“èª¿ã¸ã®é…æ…®/æ‰¹åˆ¤\n"
    "- å¤–äº¤: å¤–äº¤å§¿å‹¢ã€é˜²è¡›ã€åŒç›Ÿã€å›½éš›é–¢ä¿‚\n"
    "- çµŒæ¸ˆ: ç‰©ä¾¡ãƒ»è³ƒé‡‘ãƒ»æ™¯æ°—ãƒ»ç”£æ¥­ãƒ»ä¼æ¥­å‹•å‘ãƒ»å®¶è¨ˆ\n"
    "- å›½ä¼šé‹å–¶: è³ªç–‘å¿œç­”ã€ãƒ¤ã‚¸ã€ã„ã˜ã‚ã€æ™‚é–“é…åˆ†ã€æ‰‹ç¶šè«–\n"
    "- å…šæ´¾æ”¯æŒ: æ´¾é–¥ã€äººäº‹ã€ç·è£é¸ã€é¸æŒ™ã€å…šæ´¾æ”¯æŒ/ä¸æ”¯æŒã®è¡¨æ˜\n"
    "- ãƒ¡ãƒ‡ã‚£ã‚¢: å ±é“å§¿å‹¢ã€åˆ‡ã‚Šå–ã‚Šã€SNS/YouTubeã®è¨€è«–ç’°å¢ƒ\n"
    "- å€«ç†: ã‚¹ã‚­ãƒ£ãƒ³ãƒ€ãƒ«ã€é‡‘éŠ­ãƒ»åˆ©æ¨©ãƒ»ä¸ç¥¥äº‹ã€èª¬æ˜è²¬ä»»ã€å€«ç†/ã‚³ãƒ³ãƒ—ãƒ©\n"
    "- ãã®ä»–: ä¸Šè¨˜ã«å½“ã¦ã¯ã¾ã‚‰ãªã„å ´åˆã®ã¿ä½¿ç”¨\n"
    "If multiple categories seem plausible, choose the most specific non-'ãã®ä»–' category.\n\n"

    "=== OUTPUT FORMAT (strictly) ===\n"
    "Numbers MUST be plain integers -1, 0, or 1 (never use a leading plus sign like +1).\n"
    "Return a fully closed, valid JSON object on a single line. Do not stream or truncate.\n"
    "{ \"results\": [ {\"sentiment\": -1|0|1, "
    "\"topic\": \"æ”¿ç­–|äººæ ¼|å¤–äº¤|çµŒæ¸ˆ|å›½ä¼šé‹å–¶|å…šæ´¾æ”¯æŒ|ãƒ¡ãƒ‡ã‚£ã‚¢|å€«ç†|ãã®ä»–\"}, ... ] }\n\n"

    "=== EXAMPLES (in Japanese) ===\n"
    "ä¾‹1: ã€Œé«˜å¸‚ã•ã‚“ã‚’ç¶šæŠ•ã§ã€‚å¤–äº¤ã¯è©•ä¾¡ã—ã¦ã‚‹ã€â†’ sentiment=1, topic=å¤–äº¤\n"
    "ä¾‹2: ã€Œé«˜å¸‚ã•ã‚“ã€ãã ã‚‰ãªã„è³ªå•ãªã‚“ã‹ç„¡è¦–ã§ã„ã„ã§ã™ã‚ˆã€‚ã‚¢ãƒ›ã«ä»˜ãåˆã†å¿…è¦ç„¡ã„ã§ã™ã‚ˆã€"
    "â†’ sentiment=1, topic=å›½ä¼šé‹å–¶\n"
    "ä¾‹3: ã€Œé«˜å¸‚ç·ç†å¤§è‡£ã«ãƒ–ãƒ©ãƒƒã‚¯åŠ´åƒã•ã›ã¦ã‚‹ã®ã¯èª°ã ï¼ï¼ğŸ’¢ğŸ’¢ğŸ’¢ã€â†’ sentiment=1, topic=å…šæ´¾æ”¯æŒ\n"
    "ä¾‹4: ã€Œé«˜å¸‚ã‚‚å²¸ç”°ã‚‚ã©ã£ã¡ã‚‚ç„¡ç†ã€â†’ sentiment=-1, topic=å…šæ´¾æ”¯æŒ\n"
    "ä¾‹5: ã€Œç«‹æ†²ã¯ã»ã‚“ã¨ã©ãƒ¼ã§ã‚‚ã„ã„è³ªå•ã°ã‹ã‚ŠğŸ˜…ã€â†’ sentiment=0, topic=å…šæ´¾æ”¯æŒ\n"
    "ä¾‹6: ã€Œå²¸ç”°ã‚’æ”¯æŒã€æ¬¡ã‚‚å²¸ç”°ã§ã„ã„ã€(é«˜å¸‚ã¸ã®è¨€åŠãªã—) â†’ sentiment=0, topic=å…šæ´¾æ”¯æŒ\n"
    "ä¾‹7: ã€Œç‰©ä¾¡ãŒã¤ã‚‰ã„ã€‚æ”¿æ²»ã¯èª°ã§ã‚‚åŒã˜ã€â†’ sentiment=0, topic=çµŒæ¸ˆ\n"
    "ä¾‹8: ã€Œå½¼å¥³ã®æ…‹åº¦ã¯ç„¡ç†ã€(é«˜å¸‚ã‚’æŒ‡ã™æ–‡è„ˆ) â†’ sentiment=-1, topic=äººæ ¼\n"
)

import re

# ãƒã‚¸/ãƒã‚¬ã®æ‰‹å‹•æ•‘æ¸ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
_POS_PATTERNS = [
    r"é«˜å¸‚.*(æ”¯æŒ|å¿œæ´|ç¶šæŠ•|ç¶šã‘ã¦|ç·ç†ã«|ãªã£ã¦ã»ã—ã„|æ¨ã—|ã—ã‹å‹ãŸã‚“)",
    r"é«˜å¸‚ã•ã‚“?é ‘å¼µã£ã¦",
    r"æ—©è‹—ã¡ã‚ƒã‚“?é ‘å¼µã£ã¦",
    r"(ç·ç†|å½¼å¥³).*ã‹ã‚ã„ãã†",
    r"(ç·ç†|å½¼å¥³).*ä¼‘ã¾ã›ã¦ã‚ã’ã¦",
    r"(ã„ã˜ã‚|ãƒ‘ãƒ¯ãƒãƒ©).*(ã‚„ã‚ã‚|é…·ã™ã)",
    r"ã‚ˆãé ‘å¼µã£ã¦ã‚‹",
    r"å€’ã‚Œãªã„ã§",
    r"ãã ã‚‰ãªã„è³ªå•ãªã‚“ã‹ç„¡è¦–ã§ã„ã„",
    r"ã‚¢ãƒ›ã«ä»˜ãåˆã†å¿…è¦ç„¡ã„",
    r"ãƒ–ãƒ©ãƒƒã‚¯åŠ´åƒã•ã›ã¦ã‚‹",
]

_NEG_PATTERNS = [
    r"é«˜å¸‚.*(ç„¡ç†|å«Œã„|è¦ã‚‰ãªã„|ã‚„ã‚ã‚|è¾ã‚ã‚|çµ‚ã‚ã£ã¦ã‚‹|æœ€æ‚ª)",
    r"(ç·ç†|ã“ã„ã¤).*(ç„¡ç†|å«Œã„|çµ‚ã‚ã£ã¦ã‚‹|ãƒ€ãƒ¡)",
]

def _heuristic_adjust_sentiment(text: str, s: int) -> int:
    """
    GPT ãŒè¿”ã—ãŸ sentiment s (-1/0/1) ã‚’ã€
    å…¸å‹çš„ãªãƒã‚¸/ãƒã‚¬è¡¨ç¾ã«åŸºã¥ã„ã¦å¾®èª¿æ•´ã™ã‚‹å®‰å…¨å¼ã€‚
    """
    if not isinstance(text, str):
        return s
    t = text.replace(" ", "").replace("ã€€", "")

    # ã¾ãšãƒã‚¬ï¼ˆæ˜ç¢ºãª disï¼‰ã‚’å„ªå…ˆ
    for pat in _NEG_PATTERNS:
        if re.search(pat, t):
            return -1

    # æ˜ã‚‰ã‹ãªãƒã‚¸ï¼ˆå¿œæ´ãƒ»åŒæƒ…ãƒ»æ“è­·ï¼‰ã¯ +1 ã«æ•‘æ¸ˆ
    for pat in _POS_PATTERNS:
        if re.search(pat, t):
            return 1

    return s

def _build_user_prompt(items: List[str]) -> str:
    """
    ãƒãƒƒãƒåˆ†é¡ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã€‚
    """
    payload = {
        "instruction": {
            "sentiment": "æ—¥æœ¬èªæœ¬æ–‡ã®æ¥µæ€§ã‚’ -1(å¦å®š)/0(ä¸­ç«‹)/+1(è‚¯å®š)ã§åˆ¤å®š",
            "topic_enum": TOPIC_LABELS
        },
        "inputs": [{"id": i, "text": t} for i, t in enumerate(items)]
    }
    return json.dumps(payload, ensure_ascii=False)

def _call_gpt_batch(texts: List[str], model: str = DEFAULT_MODEL, max_retries: int = 3) -> List[Dict]:
    from openai import OpenAI
    client = OpenAI(api_key=OPENAI_API_KEY)
    user_prompt = _build_user_prompt(texts)

    last_err = None
    for attempt in range(1, max_retries + 1):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                response_format={"type": "json_object"},  # â† ã“ã‚Œã‚’è¿½åŠ 
            )

            # â–¼ ã“ã“ã‹ã‚‰ã‚’ç½®ãæ›ãˆã‚‹ â–¼
            import json, re
            raw = resp.choices[0].message.content.strip()

            # å…ˆé ­+ã®é™¤å»ï¼ˆJSONã§ã¯ä¸æ­£ï¼‰
            raw = re.sub(r':\s*\+1(\b|[^0-9])', r': 1\1', raw)

            # JSONãƒ¢ãƒ¼ãƒ‰ãªã‚‰åŸºæœ¬ãã®ã¾ã¾ãƒ‘ãƒ¼ã‚¹ã§é€šã‚‹
            try:
                data = json.loads(raw)
            except Exception:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæœ€åˆã® '{' ã‹ã‚‰æœ€å¾Œã® '}' ã¾ã§ã‚’å†æŠ½å‡ºã—ã¦å†ãƒˆãƒ©ã‚¤
                m = re.search(r'\{.*\}', raw, re.DOTALL)
                if not m:
                    raise ValueError(f"JSON parse error: {raw[:300]}")
                fixed = m.group(0)
                fixed = re.sub(r':\s*\+1(\b|[^0-9])', r': 1\1', fixed)
                data = json.loads(fixed)
            # â–² ã“ã“ã¾ã§ã‚’ç½®ãæ›ãˆã‚‹ â–²

            results = data.get("results")
            if isinstance(results, dict):
                results = [results]
            elif not isinstance(results, list):
                results = [data]

            if len(results) != len(texts):
                results = (results * math.ceil(len(texts) / len(results)))[:len(texts)]

            out = []
            for r in results:
                s = r.get("sentiment", 0)
                try:
                    s = int(s)
                except Exception:
                    s = 0
                if s not in (-1, 0, 1):
                    s = 0
                t = r.get("topic", "ãã®ä»–")
                if t not in TOPIC_LABELS:
                    t = "ãã®ä»–"
                out.append({"sentiment": s, "topic": t})
            return out

        except Exception as e:
            last_err = e
            time.sleep(1.5 * attempt)

    raise RuntimeError(f"OpenAIå‘¼ã³å‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸ: {last_err}")

def _classify_with_gpt(texts: List[str], batch_size: int = 10) -> List[Dict]:
    """
    texts ã‚’ batch_size ãšã¤ GPT ã«æŠ•ã’ã€çµåˆã—ã¦è¿”ã™ã€‚
    """
    results: List[Dict] = []
    n = len(texts)
    for i in range(0, n, batch_size):
        batch = texts[i:i + batch_size]
        chunk = _call_gpt_batch(batch)
        results.extend(chunk)
    return results

def enrich(df: pd.DataFrame) -> pd.DataFrame:
    """
    æœŸå¾…ã‚«ãƒ©ãƒ : ['text','source','likes','published_at']
    è¿”å´: sentiment(-1/0/1ã‚’floatã«å¤‰æ›: -1.0/0.0/1.0), topic, date
    """
    if df is None or df.empty:
        return df

    dfx = df.copy()
    dfx['text'] = dfx['text'].astype(str).fillna("")

    # GPTã§ä¸€æ‹¬åˆ¤å®š
    gpt_out = _classify_with_gpt(dfx["text"].tolist(), batch_size=20)

    # --- ã“ã“ã‹ã‚‰ä¿®æ­£ ---
    sentiments = []
    topics = []

    for text, item in zip(dfx["text"].tolist(), gpt_out):
        raw_s = int(item.get("sentiment", 0))  # GPTå‡ºåŠ›ï¼ˆ-1/0/1ï¼‰
        # æ•‘æ¸ˆãƒ­ã‚¸ãƒƒã‚¯ã§è£œæ­£ï¼ˆ+1/âˆ’1 ã®å–ã‚Šã“ã¼ã—ã‚’é˜²ãï¼‰
        fixed_s = _heuristic_adjust_sentiment(text, raw_s)

        sentiments.append(float(fixed_s))
        topics.append(item.get("topic", "ãã®ä»–"))
    # --- ã“ã“ã¾ã§ä¿®æ­£ ---

    dfx["sentiment"] = sentiments
    dfx["topic"] = topics
    dfx["date"] = pd.to_datetime(dfx["published_at"], errors="coerce").dt.date
    return dfx


def kpi(dfx: pd.DataFrame) -> Dict[str, float]:
    if dfx is None or dfx.empty:
        return {"n_comments": 0, "pos_rate": 0.0, "neg_rate": 0.0, "avg_sentiment": 0.0}
    n = len(dfx)
    pos = (dfx['sentiment'] > 0).mean()
    neg = (dfx['sentiment'] < 0).mean()
    avg = dfx['sentiment'].mean()
    return {"n_comments": int(n), "pos_rate": float(pos), "neg_rate": float(neg), "avg_sentiment": float(avg)}

# ========= ã“ã“ã‹ã‚‰è¿½åŠ ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã®ä¸‹ã«è¿½è¨˜ï¼‰ =========
def _normalize_topic(label: str) -> str:
    """ãƒ¢ãƒ‡ãƒ«ã‚„æ—§ç‰ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¿”ã™è¡¨è¨˜ã‚†ã‚Œã‚’ã€æ–°ã—ã„ TOPIC_LABELS ã«å¯„ã›ã‚‹"""
    if not isinstance(label, str):
        return "ãã®ä»–"
    raw = label.strip()

    mapping = {
        # æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³/åˆ¥è¡¨è¨˜ â†’ æ–°ãƒ©ãƒ™ãƒ«
        "äººæ ¼/æ…‹åº¦": "äººæ ¼",
        "å¤–äº¤/å®‰å…¨ä¿éšœ": "å¤–äº¤",
        "çµŒæ¸ˆ/ç‰©ä¾¡": "çµŒæ¸ˆ",
        "ä¸é‡å…šã®æ…‹åº¦": "å›½ä¼šé‹å–¶",
        "å›½ä¼šé‹å–¶/ä¸é‡å…šã®æ…‹åº¦": "å›½ä¼šé‹å–¶",
        "æ”¿å±€/å…šæ´¾æ”¯æŒ": "å…šæ´¾æ”¯æŒ",
        "æ”¿å±€/é¸æŒ™ãƒ»å…šæ´¾æ”¯æŒ": "å…šæ´¾æ”¯æŒ",
        "ãƒ¡ãƒ‡ã‚£ã‚¢/å ±é“": "ãƒ¡ãƒ‡ã‚£ã‚¢",
        "ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ»å ±é“": "ãƒ¡ãƒ‡ã‚£ã‚¢",
        "ã‚¹ã‚­ãƒ£ãƒ³ãƒ€ãƒ«/å€«ç†": "å€«ç†",
        "ã‚¹ã‚­ãƒ£ãƒ³ãƒ€ãƒ«ãƒ»å€«ç†": "å€«ç†",
    }

    cand = mapping.get(raw, raw)
    return cand if cand in TOPIC_LABELS else "ãã®ä»–"

def _summarize_transcript_for_context(transcript_text: str, model: str = DEFAULT_MODEL) -> str:
    """
    é•·å°ºãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’åˆ†é¡ã«åŠ¹ãæ—¥æœ¬èªè¦ç´„(<=1500æ–‡å­—ç¨‹åº¦)ã«åœ§ç¸®ã€‚
    â€» ã“ã“ã¯æ–°è¦APIå‘¼ã³å‡ºã—ï¼ˆã‚µãƒãƒªç”¨ï¼‰ã€‚ç²¾åº¦ã‚’å„ªå…ˆã€‚é€Ÿåº¦é‡è¦–ãªã‚‰ transcript_text[:4000] ã§ã‚‚OKã€‚
    """
    from openai import OpenAI
    client = OpenAI(api_key=OPENAI_API_KEY)
    prompt = (
        "ä»¥ä¸‹ã¯æ—¥æœ¬èªã®å‹•ç”»æ–‡å­—èµ·ã“ã—ã§ã™ã€‚é«˜å¸‚é¦–ç›¸ã¸ã®è³›å¦åˆ†é¡ã®æ–‡è„ˆç†è§£ã«ä½¿ãˆã‚‹ã‚ˆã†ã€"
        "æ—¥æœ¬èªã§ç®‡æ¡æ›¸ãã®é‡è¦ãƒã‚¤ãƒ³ãƒˆè¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
        "ãƒ»ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯/è«–ç‚¹ãƒ»èª°ãŒèª°ã«ä½•ã‚’ä¸»å¼µ/æ‰¹åˆ¤/æ“è­·ã—ãŸã‹ãƒ»å¤–äº¤/å®‰å…¨ä¿éšœ/çµŒæ¸ˆ/äººæ ¼ãƒ»æ…‹åº¦ã®è«–ç‚¹ãƒ»"
        "é«˜å¸‚é¦–ç›¸ã«é–¢ä¿‚ã™ã‚‹å‡ºæ¥äº‹/ç™ºè¨€/è³ªå•ã®è¦æ—¨ãƒ»è¦–è´è€…ãŒåŒæƒ…/æ“è­·/æ‰¹åˆ¤ã—ãã†ãªå ´é¢ã€‚"
        "æœ€å¤§1500æ–‡å­—ä»¥å†…ã€‚ç®‡æ¡æ›¸ãã®ã¿ã€‚"
    )
    resp = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "You summarize Japanese transcripts into concise bullet points."},
            {"role": "user", "content": prompt + "\n\nã€æ–‡å­—èµ·ã“ã—ã€‘\n" + transcript_text}
        ],
        temperature=0.2,
    )
    summary = resp.choices[0].message.content.strip()
    return summary[:2000]


def _build_user_prompt_with_context(items: List[str], context_summary: str) -> str:
    """
    å†åˆ¤å®šç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼šcontext ã«ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆè¦ç´„ã‚’åŒæ¢±ã€‚
    """
    payload = {
        "instruction": {
            "task": "Re-classify stance toward Prime Minister Sanae Takaichi using the provided context.",
            "sentiment_def": "-1(opposition)/0(neutral)/+1(support)",
            "topic_enum": TOPIC_LABELS
        },
        "context": context_summary,
        "inputs": [{"id": i, "text": t} for i, t in enumerate(items)]
    }
    return json.dumps(payload, ensure_ascii=False)


def _call_gpt_batch_with_context(texts: List[str], context_summary: str,
                                 model: str = DEFAULT_MODEL, max_retries: int = 3) -> List[Dict]:
    """
    texts(ãƒãƒƒãƒ) -> [{"sentiment": -1|0|1, "topic": "<ãƒ©ãƒ™ãƒ«>"}]
    æ—¢å­˜ã® _call_gpt_batch ã¯è§¦ã‚‰ãšã€æ–‡è„ˆä»˜ãã®åˆ¥é–¢æ•°ã¨ã—ã¦å®Ÿè£…ã€‚
    """
    from openai import OpenAI
    client = OpenAI(api_key=OPENAI_API_KEY)
    user_prompt = _build_user_prompt_with_context(texts, context_summary)

    last_err = None
    for attempt in range(1, max_retries + 1):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0,
                response_format={"type": "json_object"},  # JSONãƒ¢ãƒ¼ãƒ‰
            )

            # æ—¢å­˜å®Ÿè£…ã¨åŒã˜â€œä¿é™ºä»˜ãâ€ãƒ‘ãƒ¼ã‚¹ï¼ˆé‡è¤‡OKï¼šæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã«å½±éŸ¿ã‚’ä¸ãˆãªã„ãŸã‚ã‚³ãƒ”ãƒšï¼‰
            import json, re
            raw = resp.choices[0].message.content.strip()
            raw = re.sub(r':\s*\+1(\b|[^0-9])', r': 1\1', raw)
            try:
                data = json.loads(raw)
            except Exception:
                m = re.search(r'\{.*\}', raw, re.DOTALL)
                if not m:
                    raise ValueError(f"JSON parse error: {raw[:300]}")
                fixed = m.group(0)
                fixed = re.sub(r':\s*\+1(\b|[^0-9])', r': 1\1', fixed)
                data = json.loads(fixed)

            results = data.get("results")
            if isinstance(results, dict):
                results = [results]
            elif not isinstance(results, list):
                results = [data]

            # å¿µã®ãŸã‚ä»¶æ•°åˆã‚ã›
            if len(results) != len(texts) and len(results) > 0:
                results = (results * math.ceil(len(texts) / len(results)))[:len(texts)]

            out = []
            for r in results:
                s = r.get("sentiment", 0)
                try:
                    s = int(s)
                except Exception:
                    s = 0
                if s not in (-1, 0, 1):
                    s = 0
                t = _normalize_topic(r.get("topic", "ãã®ä»–"))
                out.append({"sentiment": s, "topic": t})
            return out

        except Exception as e:
            last_err = e
            time.sleep(1.5 * attempt)

    raise RuntimeError(f"OpenAIå‘¼ã³å‡ºã—(æ–‡è„ˆä»˜ã)ã«å¤±æ•—ã—ã¾ã—ãŸ: {last_err}")


def refine_with_transcript(dfx: pd.DataFrame, transcript_text: str,
                           summarize: bool = True, batch_size: int = 10) -> pd.DataFrame:
    """
    æ–‡å­—èµ·ã“ã—ã«ã‚ˆã‚‹â€œæ–‡è„ˆå†åˆ¤å®šâ€ã‚’ã€åˆå›åˆ†é¡ã§ sentiment==0 ã®è¡Œã«ã ã‘é©ç”¨ã™ã‚‹ã€‚
    æ—¢å­˜ã‚«ãƒ©ãƒ ã‚„ä»–è¡Œã«ã¯ä¸€åˆ‡è§¦ã‚Œãªã„ï¼ˆ= æ—¢å­˜æ©Ÿèƒ½ã‚’æãªã‚ãªã„ï¼‰ã€‚
    - dfx: enrich() æ¸ˆã¿ã® DataFrameï¼ˆsentiment, topic ãŒä»˜ä¸æ¸ˆã¿ï¼‰
    - transcript_text: ã‚ãªãŸãŒè²¼ã‚‹æ–‡å­—èµ·ã“ã—å…¨æ–‡
    - summarize: True ã®å ´åˆã¯è¦ç´„ã—ã¦ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŒæ¢±ï¼ˆé•·æ–‡ã§ã‚‚å®‰å®šï¼‰
    """
    if dfx is None or dfx.empty or not transcript_text or not isinstance(transcript_text, str):
        return dfx

    mask = (dfx["sentiment"] == 0) | (dfx["sentiment"] == 0.0)
    if not mask.any():
        return dfx  # 0ãŒç„¡ã‘ã‚Œã°ä½•ã‚‚ã—ãªã„

    # æ–‡è„ˆè¦ç´„ or ãã®ã¾ã¾ä½¿ç”¨
    if summarize:
        context_summary = _summarize_transcript_for_context(transcript_text)
    else:
        # é•·ã™ãã‚‹ã¨ä¸å®‰å®šã«ãªã‚‹ãŸã‚ã€ä¿é™ºã§ä¸Šé™
        context_summary = transcript_text[:4000]

    idx = dfx.index[mask]
    texts = dfx.loc[idx, "text"].astype(str).tolist()

    # æ–‡è„ˆä»˜ãã§å†åˆ¤å®š
    reclassified = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        chunk = _call_gpt_batch_with_context(batch, context_summary)
        reclassified.extend(chunk)

    # åæ˜ ï¼š0ã®è¡Œã ã‘ã€ã‹ã¤â€œé0ã«å¤‰ã‚ã£ãŸå ´åˆã®ã¿â€ä¸Šæ›¸ãï¼ˆ= ä¿å®ˆçš„ï¼‰
    for j, row_id in enumerate(idx):
        new_s = int(reclassified[j]["sentiment"])
        new_t = _normalize_topic(reclassified[j]["topic"])
        if new_s != 0:
            dfx.at[row_id, "sentiment"] = float(new_s)
            dfx.at[row_id, "topic"] = new_t

    return dfx
# ========= è¿½åŠ ã“ã“ã¾ã§ =========

